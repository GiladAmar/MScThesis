% A) Biological Neural Networks


"Neurotalk:"
Where’s my neuron?
‘This is how the brain works’ - G. Hinton

ANN are processing structures, whether hardware or algorithmic in nature, that are modelled after the neuron network within mammalian brains. (cite)
To date ANNs have many orders of magnitude less neurons than the mammalian brains from which they are born.
However small biological neuronal systems have been simulated rather well. (cite fish brain)
Natural neurons receive input through synapses located on the dendrites of the neuron. [cite]
When the signals received surpass a certain threshold the neuron is activated and emits a signal through the axon.
Such a signal may be received at synapses and in tern activate other neurons.
In the modelling of such a neural network, the true complexity is highly abstracted.
These artificial neurons consist of inputs, like synapses, whose signal is multiplied by weights (or signal strength).

% B) Autoencoders and DBMs
        
        
        
        
Dimensionality reduction serves classification, visualization communication and storage of high dimensional data\citep{hinton2006reducing}.
A commonly used method, Principal Component Analysis (PCA), performs a transformation of the directions in the feature space to ones that have the greatest variance.
Taking only the top N axis of the PCA feature space with greatest variance allows one to reduce the number of features used while still keep most of the information.

Other data reduction algorithms exist that encodes high-dimensional data into a lower-dimensional code.
A non-linear generalization of PCA using a multilayer neural network, known as an Autoencoder, has two parts.
The encoder network transforms the high-dimensional data to a low-dimensional code; the decoder network reconstructs the original feature vector from the code\citep{hinton2006reducing}.
A deep auto-encoder is nothing more than a multi-layered auto-encoder such that the output target is the input data\citep{dengthree}.

Autoencoders start with initial random weights in both parts.
Both networks are trained together by minimizing the discrepancy between the original data and the reconstructed data.

An autoencoder with multiple hidden layers is hard to optimize as optimization schemes find poor local minima.
With small initial weights, using a method such as back-propagation, the gradients in the early layers are insignificant making optimization infeasible.
If however the original weights are close to a good solution back-propagation works well to fine-tune the weights\citep{hinton2006reducing}.

A pre-training procedure was developed that trains one layer at a time.
This procedure uses a two layer neural network called a restricted Boltzmann machine\citep{hinton2006reducing}\citep{ackley1985learning}\citep{hinton2010practical}.
Restricted Boltzmann Machines are different from perceptron type neural networks in two ways.
They are stochastic, neurons activate (become equal to one) with a probability given by the biases, weights and activations of the previous layer, and hidden layers do not connect to each other, hence `restricted'.

A single Restricted Boltzmann Machine can be viewed as a three layer neural network.
The input and output layer have the same number of neurons and the hidden layer neurons are not interconnected.
Each such layer is trained such that the input is generated by last layer.
Generating the input is not the useful aspect of this, but if you consider that the hidden layer has less neurons than the input layer, the information is going though a bottleneck forcing the RBM to find a smaller representation of the input vector to a feature vector.

The joint configuration of the (\textbf{v},\textbf{h})of the RBM has an energy function\citep{hinton2006reducing} given by

\be
E(\textbf{v},\textbf{h})=-\sum_i b_i v_i - \sum_j b_j h_j -\sum_{i,j}v_ih_jw_{ij}
\ee

where $v_i$ and $h_j$ are the values of input vector (i) and feature vector(j) (hidden layer activations), $b_i$ and $b_j$ are their biases and $w_{ij}$ is the weight between them.
The RBM can now assign a probability to every possible output through this energy function.

The probability of the network generating a training image can be increased by adjusting the weights and biases to lower the energy of that image (increasing the odds of its generation) and raise the energy of similar yet different images that the network would prefer to the real data\citep{hinton2006reducing}.


The binary state $h_j$ of each feature detector is set to one with a probability given by $\sigma (b_j+\sum_{vi} w_{ij})$, where $\sigma(x)$ is the logistic function.
Once binary states have been decided for the hidden units, then upper layer is produced by setting each $\nu_i=1$ with a  probability of $\sigma (b_i+\sum_j h_j w_{ij})$ where $b_i$ is the bias of $i$.
Now there is an 'imagined' reconstruction of the data in the final layer that the network would 'prefer' to generate.
The rule for updating the weights such that the network's reconstructions more closely match the input data is\citep{hinton2006reducing}
\be
\Delta \omega_{ij} =\epsilon (\langle\nu_i h_j\rangle_{data} - \langle\nu_i h_j\rangle_{recon})
\ee
where $\epsilon$ is the learning rate, $\langle \nu_i h_j\rangle_{data}$ is the fraction of times that pixel $i$ and feature $j$ are on together when the feature detectors are driven by data (also called the positive gradient), $\langle \nu_i h_j \rangle_{recon}$ is the corresponding fraction for 'imagined' reconstructions (called the negative gradient).
A simplified version of this same learning rule is applied to the bias terms.


A single layer would not be the best way to model data structure.
Instead, once one layer has been learned, we treat that hidden layers activities (when driven by data) as input for the next layer.
The hidden layer of the first RBM becomes the visible first layer of the next RBM\citep{hinton2006reducing}.
This layer by layer learning can be repeated indefinitely.
In fact it has been shown that adding another layer always improves the lower bound on the log probability that the model assigns to the training data - as long as the number of feature detectors per layer does not decrease and their weights are initialized asymmetrically\citep{hinton2006reducing}.
This bound does not apply when higher levels have fewer feature detectors, however a layer-by-layer pre-training algorithm is very effective at training a deep auto-encoder known as Greedy Layer-Wise Training of Deep Networks\citep{bengio2007greedy}.
Each layer in the chain finds high-order correlations between the activities of detectors in the layer below.

After having trained several layers, the entire network can be 'unfolded' making an encoder and decoder network\citep{dengthree}.

A decoder network is simply the encoder running in reverse.
These two networks will initially  use the same weights and the one is the inverse of the other.
At this stage global fine tuning using standard backpropogation can be used to optimize reconstruction, replacing the probabilistic activities with deterministic ones\citep{hinton2006reducing}.

For continuous data the first level's hidden units remain binary whilst the visible units a replaced by linear ones with Gaussian Noise [explain]
If the noise is of unit variance the update rule remains the same for hidden units but for visible it will sample from a Gaussian of unit variance and mean $b_i +\sum_j h_j w_{ij}$ \citep{hinton2006reducing}.


A very deep auto encoder without training reconstructs the average of the training data even after many iterations of backpropogation tuning\citep{hinton2006reducing}.
Auto-encoders of a single layer can learn without pre-training, however pre-training significantly reduces training time, tends to avoid local minima and increase the networks stability\citep{erhan2010does}.
Deep auto encoders may produce lower reconstruction errors than a shallow auto encoder with fewer parameters, however this is not true as the number of parameters increases.
Autoencoders in general outperform latent semantic analysis (LSA) a document retrieval algorithm based on PCA\citep{hinton2006reducing}.
Additionally they perform better than local linear embedding, another non-linear dimensionality reduction algorithm.
Pre-training helps generalization as it ensures most of the information comes from modelling the input data.
information obtained from labels is used to only slightly adjust weights found by the pre-training.
Compared with other non-parametric algorithms auto-encoders provide a map from the data to the lower dimensional code and back, can be applied to huge sources of data and total training scales linearly with the number of training samples\citep{hinton2006reducing}.

Of course this basic introduction ignores the many alterations one can make to an auto-encoder, single or multi-layered.
De-noising auto-encoders for instance operate by corrupting the input to the hidden layer. 
The corruption of data can be done via setting a fraction of the inputs to zero, the fraction of which i




self can be randomised.
Regardless of the corruption procedure, the auto-encoder is now required to output the original, uncorrupted, input despite the added noise using criteria such as KL [?] distance between original inputs and reconstructed inputs.

[generative vs discriminative fine tuning]\citep{mo2012survey}


Autoencoders have found great use as a method to map images a to a short binary code.
Similarly for coding documents, known as semantic hashing, where the flip of any one bit from $0$ to $1$ will call up another document or image very similar to the first.


Borrowing insights from the success of CNNs, Sparse Autoencoders use units that also only connect to a local feature space\citep{bengio2009advances}.

Using an autoencoder of three layers with an overcomplete hidden layer, a layer with equal to or more than the number of imput units.
Auch autoencoders can be useful as feature extractors\citep{payan2015predicting}.
An issue with an overcomplete layer is the risk that the autoencoder simply learns the identity function\citep{bengio2012practical}.
Threfore addtional constraints are rquired, such as asparity constraints\citep{poultney2006efficient}.
Sparsity constraints encourage most hidden units to be lcose to zero, and encourages the learnign of sueful filters fro convolution operations.
This is particualy usefule in order to disentangle factorzs controlling the variability in such data.

\be
\hat{s}_j =\frac{1}{N} \sum^N_{i=1} [ a_j (x^{(i)})]
\ee
where $a_j(x)$ is the activation ofg unit $j$, given input $x$ averaged over the training set of N samples.
$\hat{s}_j =s$ where $s$ is the hyper-parameter , typically close to zero, eg. $0.05$
Such a constraint is enforced byt the addtion of a penalty term to the cost fucntion. based on teh concept of Kullback-Leibler divergence
\be
\sum^h_{j=1}KL(s||\hat{s}_j)
\ee
where $h$ is the number of hidden units in the layer, $j$ the summing index of the hidden units
\be
KL(s||\hat{s}_j)=slog(\frac{s}{\hat{s}_j})+(1-s)log \frac{1-s}{1-\hat{s}_j}
\ee
qauntifies the diergence between a Bernoulli distributomn with mean $s$ and another of mean $\hat{s}_j$.
This term is $KL(s||\hat{s}_j)=0$ where $\hat{s}_j=s$, and increases as $\hat{s}_j$ moves away from $s$.
This causes $\hat{s}_j$ to prefer values close to $s$
The penalty term $\beta KL(s||\hat{s}_j)$ will be added to the cost fucntion, for example a mean squared error, whe $\beta$ is the hyper parameter controlling the relative weight of the of the term\citep{payan2015predicting}.




Cross-entorpy cost function term  commonly used fro lcassification tasks\citep{glorot2010understanding}

\be
-\frac{1}{N}\sum^N_{i=1}\sum^3_{j=1}[\mathbb{1}\{y^(i)=j\}log(h_{W,b}(x^{(i)})_j]
\ee
$j$ sums over the numebr of classes, $h_{W,b}$ the function computed by the netwrok, $x^{(i)}$ and $y^{(i)}$ the input and label of the i'th  



Without addtional constraints, an auto-encoder can learn the identity mapping, particularly if the hidden layer is as large as or larger than the input.
There are several ways this can be countered.
The use of probablasitc RBMs (discussed later), sparse coding and denoising auto-encoders.
A de-noising AE works by trying to reconctruct the clean input from a partially corrupted one.
The input $x$ becomes $\overbrace{x}$ with the addtion of a variable amout $v$ of noise.
This may coem in the form of bianry noise, the inputs are on or off, or uncorrletaed gaussian noise.
The percentage of permissible corruption $v$ .
The denoising AE is trained by finding the latent representation $\textbf{h} = f_\theta(\bar{x}) = \sigma(W\bar{x}+b)$ with wich to reconstruct the original input  $\textbf{y} = f_{\theta'}(\bar{x})=\sigma(W'h+b')$.

Boltzmann machine(BM) is a network of symmetrically connected neuron-like units which activate according to stochastic functions.
In contrast, an Restricted Boltzmann machine(RBN)is a two layer BM where it is restricted in the sense that there are no visible-visible nor hidden-hidden connections w\textbf{v} and within the same layer.
The RBM is a special case of a Markov random field having one layer of (usually Bernoulli) stochastic hidden units and one layer (either Bernoulli or Gaussian) stochastic visible units.
represented as bipartite graphs, there are no visible-visible or hidden-hidden connections.
Generally DBNs can be trained as a discriminative model (inference, classification etc..) or a generative model (generating training data, simulating etc)\citep{mo2012survey}.

In an RBM the joint distribution $p(\textbf{v,h},\theta)$ over the visible units \textbf{v} and hidden units \textbf{h} given model parameters $\theta$ is defined in terms of the energy function $E=(\textbf{v,h},\theta)$ (related to Thermodynamics[reference])\citep{dengthree}:
\be
p(\textbf{v,h},\theta)= \frac{exp(-E(\textbf{v,h},\theta))}{Z}
\ee

where Z is the normalization factor, or partition function, $Z=\sum_\textbf{v} \sum_\textbf{h} exp(-E(\textbf{v,h},\theta))$

The marginal probability that the model assigns to a visible vector $\textbf{v}$ is

\be
p(\textbf{v},\theta)  = \frac{\sum_\textbf{h} exp(-E(\textbf{v,h},\theta}{Z}
\ee

For a Bernoulli(visible to hidden) RBM the energy function is defined as 
\be
E(\textbf{v,h},\theta) = -\sum^I_{i=1} \sum^J_{j=1} w_{ij} v_i h_j - \sum^I_{i=1}b_iv_i - \sum^J_{j=1} a_j h_j 
\ee

where $W_{ij}$ is the symmetric interaction term between visible unit $v_i$ and hidden unit $h_j$, $b_i$ and $a_j$ are the bias terms with $I$ and $J$ are the numbers of visible and hidden units respectively.

It follows that conditional probabilities are then:
\be
p(h_j=1|\textbf{v};\theta)=\sigma(\sum^I_{i=1}w_{ij}v_i +a_j)
\ee

and
\be
p(v_i=1|\textbf{h};\theta)=\sigma(\sum^J_{j=1}w_{ij}h_j +b_i)
\ee

where $\sigma(x)=\frac{1}{1+exp(x)}$


In the case of Gaussian visible units to Bernoulli hidden ones:

\be
E(\textbf{v,h},\theta) = -\sum^I_{i=1} \sum^J_{j=1} w_{ij} v_i h_j - \frac{1}{2} \sum^I_{i=1}(v_i-b_i)^2 - \sum^J_{j=1} a_j h_j 
\ee

changing the conditional probabilities to
\be
p(h_j=1|\textbf{v};\theta)=\sigma(\sum^I_{i=1}w_{ij}v_i +a_j)
\ee

and
\be
p(v_i=1|\textbf{h};\theta)=N(\sum^J_{j=1}w_{ij}h_j +b_i, 1)
\ee
which is to say $v_i$ takes real values from a Gaussian distribution of mean $\sum^J_{j=1}w_{ij}h_j +b_i$ and variance $1$.
A Gaussian Bernoulli RBM is useful to convert real valued stochastic variables to binary stochastic variables, which can then be further utilised by Bernoulli-Bernoulli RBMs.

Other types of conditional distributions for the visible units ij the RBM can also be used[wleling et all 2005].

Taking the gradient of the log-likelihood log $p(\textbf{v},\theta)$ an update rule may be derived as 

\be
\delta w_{ij}=E_{data} (v_i h_j) - E_{model} (v_i h_j)
\ee

where $E_{data}(v_ih_j)$ is the expectation observed in the training set and $E_{model}(v_ih_j)$ the same expectation under the distribution under the distribution defined by the model.
However $E_{data}(v_ih_j)$ is intractable to compute and so the contrastive divergence (CD) approximation to the gradient is used where $E_{model}(v_ih_j)$ is replaced by a Gibbs sampler initialised at eh data for a full step.
Steps are as follows:

Initialize $V_0$ at data

Sample $\textbf{h}_0 \sim \textbf{p(h}\vert \textbf{v}_0 ) $
Sample $\textbf{v}_1 \sim \textbf{p(v}\vert \textbf{h}_0)$
Sample $\textbf{h}_1 \sim \textbf{p(h}\vert \textbf{v}_1)$

We have $\textbf{v}_1\textbf{,h}_1$ which serves as a rough estimate of $E_{model} (v_ih_j) = (\textbf{v}_\infty\textbf{,h}_\infty\textbf{)}$ which is a true sample from the model.
The approximation to $E_{model}(v_i,h_j)$ leads to the one-step algorithm of CD-1.

Similar to Sparse-Autoencoders, Sparse RBMs too can be made except trained by contrastive divergence rather than backpropogation
{bengio2009advances}. 





DBNs(slakhutdinov and hinton 2009,2012)are probabilistic generative models composed of multiple layers of stochastic hidden nodes/variables.
A DBN of only one layer is just as RBM.
Composing many RBMs the hidden layers can be efficiently learned using feature activations of the RBM below as input training data for the next producing a Deep Belief Network (DBN)\citep{dengthree}.
The top two layers have undirected, symmetric connections between them whereas the lower layers receive top-down, directed connections from the layer above\citep{dengthree}.
The ability to learn increasingly complex representations of the data make them apt for object and speech recognition.
Furthermore a large supply  of unlabelled data is enough to build a DBN with a little labelled data required to fine tune it\citep{dengthree}.
This is particularly useful for  image and speech recognition where the supply of unlabelled data is nearly unlimited\citep{dengthree}\citep{lecun1995convolutional}.


Deep Belief Networks employ deep architecture to learn from labelled and unlabelled data effectively\citep{dengthree}.
It makes use of an unsupervised pre-training stage followed by a supervised fine-tuning to build the models\citep{chen2014big}.
Typically a DBN is a stack of Restricted Boltzmann Machines(RBMs) followed by additional layer/s for discriminative tasks\citep{dengthree}.
RBMs are probabilistic generative models that learn a joint probability distribution of the training data without the use of labels.
An RBM consists of two layers, nodes in the one layer are fully connected to nodes in the next\citep{chen2014big}, but there are no connections between nodes of the same layer.

The entire stack of RBMs is trained layer by layer.
Once the bottom layer has been trained -Gaussian-Bernoulli for applications with continuous features, or Bernoulli-Bernoulli for applications with binary features - , it's activation probabilities of it's hidden units  will be the input of the next layer (Bernoulli-Bernoulli) where that layer is now trained\citep{dengthree}.
The training procedure repeats till all RBM's are trained.
The layer by layer training avoids local optima and alleviates over-fitting\citep{dengthree}.
In addition the algorithm is very time efficient which scales linearly with the number and size of the RBMs\citep{chen2014big}.
Upper layer RBMs contain higher level features constructed from lower level ones from the layer below.
In addition the hidden variables in the deepest layer are efficient to compute\citep{dengthree}.
This stacking procedure given by [Hinton et al 2006] improves the variational lower bound on the likelihood of the training data under the composite model, which is to say it achieves maximum likelihood training.



A simple RBM with a Bernoulli distribution for both visible and hidden layers samples from the probability distributions\citep{chen2014big}:

\be
p(h_j=1|\mathbf{v}; W) = \sigma (\sum^I_{j=1} w_{ij}\nu_{i}+a_j)
\ee
and
\be
p(v_i=1|\mathbf{h}; W) = \sigma (\sum^I_{j=1} w_{ij}h_{j}+b_i)
\ee
where $\mathbf{v}$ and $\mathbf{h}$ represent a $I\times1$ visible unit vector and a $J\times 1$ hidden unit vector respectively.
W is the matrix of weights connecting visible and hidden layers.
$a_j$ and $b_i$ are the bias terms and $\sigma(\dot{})$ is the sigmoid function.

Where the visible units are real-valued a Gaussian-Bernoulli distribution is typically assumed, $p(v_i\vert \mathbf{h};W)$ is Gaussian.
Weights are updated with an approximation method developed by Hinton called  contrastive divergence (CD) to avoid the issue of computing the log likelihood gradient\citep{mo2012survey}.
CD is efficient enough to be practical\citep{mo2012survey}.
The $n+1$-th weight will be updated as\citep{chen2014big}:

\be
\Delta w_{ij}^{(n+1)}=c w_{ij}^(n) + \alpha (\langle v_i h_j \rangle_{data} - \langle_i h_j \rangle_{model})
\ee
where $\alpha$ is the learning rate, $c$ the momentum factor, $\langle \dot{} \rangle_{data}$ and $\langle \dot{} \rangle_{model}$ are the expections under the distributions defined by the data and model respectively.
Expectations may be calculated by using Gibbs samplings infinitely many times, in practice however one-step CD is used as it performs well.
Other model parameters such as the bias terms can be updated in a similar manner.

In a generative mode the RBM training involves  a Gibbs sampler to sample hidden units from visible units and vice versa using the two equations [label them]\citep{mo2012survey}.
The weights are then updated through the CD rule and through the the process repeated till convergence.

After pre-training input data is modelled by the weights between adjacent layers \citep{chen2014big}.
A final layer is added on top to represent the output classifications where the entire network is now fine-tuned using labelled data and backpropgation.
What goes to the top layer depends on the application.
For speech recognition problems the output can represent syllables, phones, and sub-phones or phone states or speech units used in the HMM-based speech recognition system\citep{dengthree}.


Some implementations put another layer on top of the RBMs called associative memory also determined by supervised learning methods\citep{chen2014big}.


There are many variations for pre-training procedure.
Instead of using RBMs some use stacked de-noising auto-encoders or stacked predictive sparse coding\citep{chen2014big}.

Recent research has shown that when using large amounts of labelled data,a randomised initial neural network can also 
very well without pre-training.
As an example, a network of a single hidden layer is trained via backpropgation till convergence.
Once completed a new layer is inserted between the output layer and the first hidden layer and again trained to convergence and so on until the previously defined number of hidden layers has been reached\citep{chen2014big}.
Using a DBN to initialize the weights of a correspondingly configured  MLP reliably produces better results than an MLP of randomly initialised weights.
The resulting network of an DNN initialized via a DBN is called a DBN-DNN\citep{dengthree}.

In summary, DBN's use a greedy layer by layer approach to learn weights for each hidden layer and then fine-tune using backpropogation.
This has been shown to increase performance\citep{chen2014big}\citep{jarrett2009best}.
Such greedy-learning training-time increases linearly with the size and depth of the Network\citep{dengthree}.

Insights from DBN greedy training have inspired advances in standard DNNs.
Training DNN's alternatively layer by layer by considering each pair of layers as a de-noising auto-encoder regularized by setting a subset of the inputs to zero.

After the effectiveness of DBN training was established, new methods of doing pre-training were developed. 
One can learn a DNN by starting with a shallow neural network.
Having trained this layer discriminitely - using early stops to avoid over-fitting-, a new layer is inserted between the previous hidden layer and the softmax output layer where the network is again discriminatively trained.
This process is continued till the desired number of layers, or desired accuracy measures are met.
Once the full number of layers have been trained this way a full backpropogation fine-tuning is carried out to further enhance the network\citep{dengthree}.

There are many plausible variants such as a combination of a DBN with a CNN, forming a Convolutional Deep Belief Network (CDBNs) where there’s a pooling layer inserted between adjacent RBM layers in a DBN\citep{mo2012survey}.
CDBNs are useful at dealing with images having large dimensions, performing better than DBNs\citep{mo2012survey}.

% C) Data Reduction

% D) Big Data

% E) Code

% F)